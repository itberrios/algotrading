{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Models\n",
    "\n",
    "In this notebook, we will explore Transformer Architectures to perform classification of price movement on stock data. We will preprocess the stock data to contain the Times in the form of sines and cosines in order to feed additional data to our model. We will follow the approach outlined in the [paper](https://arxiv.org/pdf/2010.02803.pdf), where the features are projected into high dimensional space and a time/sequence representation is learned by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_generator import WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for python scripts use: \"os.path.dirname(__file__)\" instead of \"os.path.abspath('')\"\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.abspath(''), os.path.pardir)))\n",
    "\n",
    "from data_clean import get_trading_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'..\\data\\raw\\AAPL_15min.csv'\n",
    "df = pd.read_csv(data_path, index_col=0, \n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# df = get_trading_times(df)\n",
    "df = df.dropna()\n",
    "\n",
    "# add days, hours, and minutes to the dataset\n",
    "dayofweek = df.index.dayofweek\n",
    "hour = df.index.hour\n",
    "minute = df.index.minute\n",
    "\n",
    "# encode the days, hours, and minutes with sin and cos functions\n",
    "days_in_week = 7\n",
    "hours_in_day = 24\n",
    "minutes_in_hour = 60\n",
    "\n",
    "df['sin_day'] = np.sin(2*np.pi*dayofweek/days_in_week)\n",
    "df['cos_day'] = np.cos(2*np.pi*dayofweek/days_in_week)\n",
    "df['sin_hour'] = np.sin(2*np.pi*hour/hours_in_day)\n",
    "df['cos_hour'] = np.cos(2*np.pi*hour/hours_in_day)\n",
    "df['sin_minute'] = np.sin(2*np.pi*minute/minutes_in_hour)\n",
    "df['cos_minute'] = np.cos(2*np.pi*minute/minutes_in_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add target columns\n",
    "We will add a column for price change at each interval, this will be our regression target variable. We will also add another column that quantifys the magnitude of the price change, this will be out target variable for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itber\\AppData\\Local\\Temp\\ipykernel_29180\\2560370332.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['price_change'][df['price_diff'] < -thresh] = 0 # downward price movement\n",
      "C:\\Users\\itber\\AppData\\Local\\Temp\\ipykernel_29180\\2560370332.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['price_change'][df['price_diff'] > thresh] = 2 # upward prive movement\n"
     ]
    }
   ],
   "source": [
    "df['price_diff'] = df['close'].diff()\n",
    "\n",
    "thresh = 0.1 # dollars\n",
    "df['price_change'] = 1 # price stays the same\n",
    "df['price_change'][df['price_diff'] < -thresh] = 0 # downward price movement\n",
    "df['price_change'][df['price_diff'] > thresh] = 2 # upward prive movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>sin_minute</th>\n",
       "      <th>cos_minute</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>price_change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-01 04:30:00</th>\n",
       "      <td>115.634512</td>\n",
       "      <td>115.792604</td>\n",
       "      <td>115.407254</td>\n",
       "      <td>115.407254</td>\n",
       "      <td>13550.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.207496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 04:45:00</th>\n",
       "      <td>115.367731</td>\n",
       "      <td>115.367731</td>\n",
       "      <td>115.120712</td>\n",
       "      <td>115.308447</td>\n",
       "      <td>12857.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.098808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 05:00:00</th>\n",
       "      <td>115.308447</td>\n",
       "      <td>115.397374</td>\n",
       "      <td>115.298566</td>\n",
       "      <td>115.318327</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 05:15:00</th>\n",
       "      <td>115.417135</td>\n",
       "      <td>115.604869</td>\n",
       "      <td>115.377612</td>\n",
       "      <td>115.604869</td>\n",
       "      <td>3534.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.832769e-16</td>\n",
       "      <td>0.286542</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 05:30:00</th>\n",
       "      <td>115.604869</td>\n",
       "      <td>115.703677</td>\n",
       "      <td>115.555466</td>\n",
       "      <td>115.703677</td>\n",
       "      <td>7688.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.098808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           open        high         low       close   volume  \\\n",
       "time                                                                           \n",
       "2020-10-01 04:30:00  115.634512  115.792604  115.407254  115.407254  13550.0   \n",
       "2020-10-01 04:45:00  115.367731  115.367731  115.120712  115.308447  12857.0   \n",
       "2020-10-01 05:00:00  115.308447  115.397374  115.298566  115.318327  10079.0   \n",
       "2020-10-01 05:15:00  115.417135  115.604869  115.377612  115.604869   3534.0   \n",
       "2020-10-01 05:30:00  115.604869  115.703677  115.555466  115.703677   7688.0   \n",
       "\n",
       "                      sin_day   cos_day  sin_hour  cos_hour    sin_minute  \\\n",
       "time                                                                        \n",
       "2020-10-01 04:30:00  0.433884 -0.900969  0.866025  0.500000  5.665539e-16   \n",
       "2020-10-01 04:45:00  0.433884 -0.900969  0.866025  0.500000 -1.000000e+00   \n",
       "2020-10-01 05:00:00  0.433884 -0.900969  0.965926  0.258819  0.000000e+00   \n",
       "2020-10-01 05:15:00  0.433884 -0.900969  0.965926  0.258819  1.000000e+00   \n",
       "2020-10-01 05:30:00  0.433884 -0.900969  0.965926  0.258819  5.665539e-16   \n",
       "\n",
       "                       cos_minute  price_diff  price_change  \n",
       "time                                                         \n",
       "2020-10-01 04:30:00 -1.000000e+00   -0.207496             0  \n",
       "2020-10-01 04:45:00 -1.836970e-16   -0.098808             1  \n",
       "2020-10-01 05:00:00  1.000000e+00    0.009881             1  \n",
       "2020-10-01 05:15:00  2.832769e-16    0.286542             2  \n",
       "2020-10-01 05:30:00 -1.000000e+00    0.098808             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Standardized train, valid, and test sets\n",
    "\n",
    "Split into train, valid, and test sets. And then standardize with training mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16112, 13)\n",
      "(9243, 13)\n",
      "(6267, 13)\n"
     ]
    }
   ],
   "source": [
    "train_df = df.loc['2020-10-01':'2021-10-01']\n",
    "valid_df = df.loc['2021-10-02':'2022-05-01']\n",
    "test_df = df.loc['2022-05-02':]\n",
    "\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "# ensure that target column is not standardized\n",
    "train_mean.price_change = 0\n",
    "train_std.price_change = 1\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "valid_df = (valid_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Generator for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = WindowGenerator(\n",
    "                input_width=24, label_width=1, shift=1, \n",
    "                train_df=train_df, valid_df=valid_df, test_df=test_df,\n",
    "                label_columns=['price_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 24, 13)\n",
      "Targets shape (batch, time, features): (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in data_gen.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {inputs.shape}')\n",
    "    print(f'Targets shape (batch, time, features): {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Start Training Models**\n",
    "\n",
    "First we will define a helper function to streamline this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, lr=1e-4, max_epochs=100, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(window.train, epochs=max_epochs,\n",
    "                        validation_data=window.valid,\n",
    "                        callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, n_heads, d_k, d_v, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        num_heads=n_heads, key_dim=d_k, value_dim=d_v, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model(\n",
    "            input_shape,\n",
    "            n_heads,\n",
    "            d_k,\n",
    "            d_v,\n",
    "            ff_dim,\n",
    "            num_transformer_blocks,\n",
    "            mlp_units,\n",
    "            n_outputs=1,\n",
    "            dropout=0.1,\n",
    "            mlp_dropout=0.1,\n",
    "        ):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, n_heads, d_k, d_v, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_outputs, activation='softmax')(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model with Keras classes\n",
    "\n",
    "Things to try:\n",
    "- Try to replace Layer Normalization with Batch Normalization and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_heads, head_size, ff_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attn_heads = list()\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # print(input_shape)\n",
    "        \n",
    "        # attention portion\n",
    "        # self.attn_multi = layers.MultiHeadAttention(self.n_heads, \n",
    "        #                                             self.d_k, \n",
    "        #                                             self.d_v, \n",
    "        #                                             dropout=self.dropout)\n",
    "        self.attn_multi = MultiHeadAttention(num_heads=self.n_heads, \n",
    "                                             head_size=self.head_size, \n",
    "                                             dropout=self.dropout)\n",
    "        self.attn_dropout = layers.Dropout(self.dropout)\n",
    "        self.attn_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # feedforward portion\n",
    "        self.ff_conv1 = layers.Conv1D(filters=self.ff_dim, \n",
    "                                      kernel_size=1, \n",
    "                                      activation='relu')\n",
    "        self.ff_dropout = layers.Dropout(self.dropout)\n",
    "        self.ff_conv2 = layers.Conv1D(filters=input_shape[-1],\n",
    "                                      kernel_size=1)\n",
    "        self.ff_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # attention portion\n",
    "        x = self.attn_multi([inputs, inputs])\n",
    "        x = self.attn_dropout(x)\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        # get first residual\n",
    "        res = x + inputs\n",
    "        \n",
    "        # feedforward portion\n",
    "        x = self.ff_conv1(res)\n",
    "        x = self.ff_dropout(x)\n",
    "        x = self.ff_conv2(x)\n",
    "        x = self.ff_norm(x)\n",
    "        \n",
    "        # return residual\n",
    "        return res + x\n",
    "    \n",
    "    # Needed for saving and loading model with custom layer\n",
    "    def get_config(self): \n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout': self.dropout_rate})\n",
    "        return config          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "            n_heads,\n",
    "            head_size,\n",
    "            ff_dim,\n",
    "            num_transformer_blocks,\n",
    "            mlp_units,\n",
    "            n_outputs=3,\n",
    "            dropout=0.1,\n",
    "            mlp_dropout=0.1):\n",
    "            \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.n_outputs = n_outputs\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "\n",
    "        # self.encoders = [TransformerEncoder(n_heads, head_size, ff_dim, dropout) \n",
    "        #                  for _ in range(num_transformer_blocks)]\n",
    "         \n",
    "    def call(self, x):\n",
    "\n",
    "        # project input data into high dimensional space\n",
    "        x = layers.Dense(self.head_size)(x)\n",
    "        \n",
    "        # Encoder Portion\n",
    "        # # for encoder in self.encoders:\n",
    "        # #     x = encoder(x)\n",
    "        # for _ in range(self.num_transformer_blocks):\n",
    "        #     x = TransformerEncoder(self.n_heads, self.head_size, self.ff_dim, self.dropout)(x)\n",
    "\n",
    "        # # Averal\n",
    "        # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "\n",
    "        # # MLP portion for classification\n",
    "        # for dim in self.mlp_units:\n",
    "        #     x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        #     x = layers.Dropout(self.mlp_dropout)(x)\n",
    "        # outputs = layers.Dense(self.n_outputs, activation='softmax')(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_shape,\n",
    "            n_heads,\n",
    "            head_size,\n",
    "            ff_dim,\n",
    "            num_transformer_blocks,\n",
    "            mlp_units,\n",
    "            n_outputs=3,\n",
    "            dropout=0.1,\n",
    "            mlp_dropout=0.1,\n",
    "        ):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # model to project inputs into higher dimensional space (same as head_size??)\n",
    "    # This is just a linear layer with a bias and no activation\n",
    "    x = layers.Dense(units=head_size)(x)\n",
    "\n",
    "    # model to encode time/positions into high dimensional data\n",
    "\n",
    "    # encoder portion\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = TransformerEncoder(n_heads, head_size, ff_dim, dropout)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "\n",
    "    # MLP portion for classification\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_outputs, activation='softmax')(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = inputs.shape[1:]\n",
    "\n",
    "transformer_model = build_transformer(\n",
    "    input_shape,\n",
    "    n_heads=2,\n",
    "    head_size=512,\n",
    "    ff_dim=256,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[256],\n",
    "    n_outputs=3,\n",
    "    dropout=0.1,\n",
    "    mlp_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TransformerModel(\n",
    "            n_heads=2,\n",
    "            head_size=512,\n",
    "            ff_dim=256,\n",
    "            num_transformer_blocks=2,\n",
    "            mlp_units=[256],\n",
    "            n_outputs=3,\n",
    "            dropout=0.1,\n",
    "            mlp_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_and_fit(transformer_model, data_gen, patience=15, max_epochs=5)\n",
    "\n",
    "val_performance['transformer'] = transformer_model.evaluate(data_gen.valid)\n",
    "performance['transformer'] = transformer_model.evaluate(data_gen.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b2da39daaaff30a84159e8452ba91acfc0bbd521fb66c6aa9941f847b87bd81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
